{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required packages\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk, re, pprint\n",
    "import gensim\n",
    "import numpy\n",
    "import csv\n",
    "import fuzzywuzzy\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the working directory\n",
    "\n",
    "\n",
    "#Print current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "#Change working directory to desired location\n",
    "os.chdir(\"/Users/preshantjoseph/Documents/University/Data Science Master Class 1/Assignment 4\")\n",
    "\n",
    "#Confirm working directory has been changed to desired locatio\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import field of occupation scrape for inputting to model\n",
    "\n",
    "data = pd.read_csv(\"occupations.csv\", encoding = 'latin1', header = 0)\n",
    "\n",
    "occ_data = data\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for duplicate rows using the page urls \n",
    "\n",
    "#1: print all rows where the ID is one of the IDs in duplicated:\n",
    "\n",
    "URL = data[\"page\"]\n",
    "Duplicates = data[URL.isin(URL[URL.duplicated()])]\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix UNIT GROUP 2341 AGRICULTURAL AND FORESTRY SCIENTISTS record by concatenating occ text\n",
    "\n",
    "#select duplicate records related to unit group 2341\n",
    "occ_text_112 = Duplicates[Duplicates['page'] ==\"https://www.abs.gov.au/ausstats/abs@.nsf/Latestproducts/B2B7F0A9B46005E3CA2584A8000E7A83?opendocument\"]\n",
    "\n",
    "#Select first record to be base\n",
    "occ_text_112_fix = occ_text_112.iloc[0,:]\n",
    "\n",
    "#Combine the occupation text of all three records\n",
    "occ_text_112_fix['occ_text'] = occ_text_112.iloc[0,2] + occ_text_112.iloc[1,2] + occ_text_112.iloc[2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix UNIT GROUP 4523 SPORTS COACHES, INSTRUCTORS AND OFFICIALS record by concatenating occ text\n",
    "\n",
    "#select duplicate records related to unit group 4523\n",
    "occ_text_331 = Duplicates[Duplicates['page'] ==\"https://www.abs.gov.au/ausstats/abs@.nsf/Latestproducts/25A2FCFBA49A8022CA2584A8000E7A1A?opendocument\"]\n",
    "\n",
    "#Select first record to be base\n",
    "occ_text_331_fix = occ_text_331.iloc[0,:]\n",
    "\n",
    "#Combine the occupation text of both records\n",
    "occ_text_331_fix['occ_text'] = occ_text_331.iloc[0,2] + occ_text_331.iloc[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicate data and reinstert fixed data\n",
    "\n",
    "#keep the 2341 combined data\n",
    "keepdata = pd.DataFrame(occ_text_112_fix).T\n",
    "\n",
    "#Append with combined 4523 data\n",
    "keepdata = keepdata.append(occ_text_331_fix)\n",
    "\n",
    "#drop duplicate rows from data set\n",
    "\n",
    "#drop rows which have 2341 data \n",
    "occ_data = occ_data[occ_data.page != 'https://www.abs.gov.au/ausstats/abs@.nsf/Latestproducts/B2B7F0A9B46005E3CA2584A8000E7A83?opendocument']\n",
    "\n",
    "#drop rows which have 4523 data \n",
    "occ_data = occ_data[occ_data.page != 'https://www.abs.gov.au/ausstats/abs@.nsf/Latestproducts/25A2FCFBA49A8022CA2584A8000E7A1A?opendocument']\n",
    "\n",
    "#append occ data with fixed rows for occupations 2341 and 4523\n",
    "occ_data = occ_data.append(keepdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only 4 digit data from scraped data \n",
    "\n",
    "#Extract numeric occupation codes from the occ titles\n",
    "\n",
    "#select occ title column from occ dataframe\n",
    "occ_titles = occ_data['occ_title']\n",
    "\n",
    "#loop through occ_titles df to extract all occupation codes\n",
    "occ_codes = []\n",
    "\n",
    "for i in occ_titles:\n",
    "    occ_code = int(''.join(filter(str.isdigit, i)))\n",
    "    occ_codes.append(occ_code)\n",
    "\n",
    "#Add occ codes to occ_data data frame\n",
    "occ_data['occupation code'] = occ_codes\n",
    "    \n",
    "    \n",
    "#Create new column with ANZSCO level\n",
    "\n",
    "#loop through occ codes to find length of numeric variable and determine ANZSCO classification level\n",
    "level = []\n",
    "\n",
    "for i in occ_codes:\n",
    "    length = len(str(i))\n",
    "    level.append(length)\n",
    "\n",
    "#Add ANZSCO classification level to occ_data data frame\n",
    "occ_data['ANZSCO_classification_level'] = level\n",
    " \n",
    "# Keep only 4 digit occupations to form base of text similarity\n",
    "\n",
    "occ_data_4dig = occ_data[occ_data.ANZSCO_classification_level == 4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine occupation titles and scraped occupation text to create associated words for text similarity\n",
    "\n",
    "occ_text_4digdata = occ_data_4dig\n",
    "occ_text_4digdata[\"Associated words\"] = occ_data_4dig[\"occ_title\"] + ', ' + occ_data_4dig[\"occ_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory data analysis on 4 digit occupation dataset\n",
    "\n",
    "#Preliminary exploration of the text\n",
    " \n",
    "word_token = []\n",
    "for i in occ_text_4digdata['Associated words']:\n",
    "    word_token = word_tokenize(i)    \n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(word_token)\n",
    "print(fdist)\n",
    "\n",
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "fdist.plot(34,cumulative=False)\n",
    "%matplotlib inline\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dataset for Text similarity analysis\n",
    "\n",
    "occ4_base_data = occ_text_4digdata[['occupation code', 'Associated words']] #select occupation code and associated words\n",
    "occ4_spark = occ_text_4digdata[['occupation code', 'Associated words']]\n",
    "\n",
    "#Convert words to lower case and reduce sparsity\n",
    "occ4_base_data['Associated words'] = occ4_base_data['Associated words'].str.lower()\n",
    "\n",
    "occ4_base_data['Associated words'] = occ4_base_data['Associated words'].astype(str)\n",
    "\n",
    "#Word tokenization\n",
    "occ4_base_data['Associated words']  = [word_tokenize(row) for row in occ4_base_data['Associated words']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin preparing the occupation data\n",
    "\n",
    "#Text processing the occupation data\n",
    "\n",
    "start_time = time.monotonic() #timing of code starts\n",
    "\n",
    "#Lemmatization\n",
    "\n",
    "#set lemma to call wordnet lemmatizer function\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "#Create a function to more easily lemmatize text\n",
    "def lemma_list(row):\n",
    "    my_list = row['Associated words']\n",
    "    lemma_list = [lemma.lemmatize(word) for word in my_list]\n",
    "    return (lemma_list)\n",
    "\n",
    "#Using lemmatize function on the associated words text\n",
    "occ4_base_data['Associated words'] = occ4_base_data.apply(lemma_list, axis = 1)\n",
    "\n",
    "\n",
    "#Stemming words\n",
    "\n",
    "#set stemming to call porterstemmer function\n",
    "stemming = PorterStemmer()\n",
    "\n",
    "#Create a function to more easily stem words\n",
    "def stem_list(row):\n",
    "    my_list = row['Associated words']\n",
    "    stemmed_list = [stemming.stem(word) for word in my_list]\n",
    "    return (stemmed_list)\n",
    "\n",
    "#Apply stemming function on the associated words text\n",
    "occ4_base_data['Associated words'] = occ4_base_data.apply(stem_list, axis = 1)\n",
    "\n",
    "#Remove stop words from text\n",
    "\n",
    "#set stops to call stop words function using english dictionary\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "#Create a function to more easily remove stop words\n",
    "def remove_stops(row):\n",
    "    my_list = row['Associated words']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "#Apply remove stop words function on the associated words text\n",
    "occ4_base_data['Associated words'] = occ4_base_data.apply(remove_stops, axis=1)\n",
    "\n",
    "#Remove numbers and punctuation\n",
    "\n",
    "#Create a function to more easily remove numbers and punctuation\n",
    "def remove_num_punc(row):\n",
    "    review = row['Associated words']\n",
    "    # taken only words (not punctuation)\n",
    "    token_words = [w for w in review if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "#Apply remove numbers and punctuation function on the associated words text\n",
    "occ4_base_data['Associated words'] = occ4_base_data.apply(remove_num_punc, axis=1)\n",
    "\n",
    "#Create dictionary for text similarity using gensim package\n",
    "dictionary = gensim.corpora.Dictionary(occ4_base_data['Associated words'])\n",
    "\n",
    "#Create bag of words\n",
    "corpus = [dictionary.doc2bow(entry) for entry in occ4_base_data['Associated words']]\n",
    "\n",
    "#Creating Term Frequency - Inverse Document Frequency (TFIDF)\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "\n",
    "#Creating similarity measure object\n",
    "sims = gensim.similarities.Similarity(os.getcwd(),tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "\n",
    "#calculating similarities for all 358 occupations at the 4 digit occupation level\n",
    "occ_doc_tf_idf = tf_idf[corpus]\n",
    "\n",
    "#Apply similaity measure object to TFIDF\n",
    "similarities = (sims[occ_doc_tf_idf])\n",
    "\n",
    "\n",
    "#time measurement\n",
    "end_time = time.monotonic() #Time measurement finish\n",
    "print(timedelta(seconds=end_time - start_time)) #print end time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find top 10 occupations with the highest similarity score and return index position\n",
    "\n",
    "#Return top 11 occupations with highest similarity score for each occupation\n",
    "idx = (-similarities).argsort(axis = 0)[:11]\n",
    "\n",
    "#Convert top occupation similarity scores to dataframe\n",
    "idx_df = pd.DataFrame(idx).T\n",
    "\n",
    "#Drop the maximum similarity score as this is just the similarity score for its own occupation\n",
    "idx_df = idx_df.drop(idx_df.columns[0], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary to map index values to occupation codes\n",
    "\n",
    "#Form the base of the dictionary\n",
    "occ_dictionary = occ_text_4digdata[['occ_title','occupation code']]\n",
    "\n",
    "#Create an index variable which will correspond with the index variables in idx_df\n",
    "index = [i for i in range(len(occ_dictionary))]\n",
    "\n",
    "#Add index variable to the base dictionary which was created\n",
    "occ_dictionary['index'] = index\n",
    "\n",
    "#Set the index variable as the dictionary index\n",
    "occ_dictionary.set_index(\"index\", drop=True, inplace=True) #only run once\n",
    "\n",
    "#convert dictionary array into actual dictionary setting the index as index\n",
    "occ_dictionary = occ_dictionary.to_dict(orient=\"index\") #only be run once\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map all the columns in idx_df to corresponding occ title and occ code\n",
    "mapped_col0 = idx_df.iloc[:,0].map(occ_dictionary)#remap column 0\n",
    "mapped_col1 = idx_df.iloc[:,1].map(occ_dictionary)#remap column 1\n",
    "mapped_col2 = idx_df.iloc[:,2].map(occ_dictionary)#remap column 2\n",
    "mapped_col3 = idx_df.iloc[:,3].map(occ_dictionary)#remap column 3\n",
    "mapped_col4 = idx_df.iloc[:,4].map(occ_dictionary)#remap column 4\n",
    "mapped_col5 = idx_df.iloc[:,5].map(occ_dictionary)#remap column 5\n",
    "mapped_col6 = idx_df.iloc[:,6].map(occ_dictionary)#remap column 6\n",
    "mapped_col7 = idx_df.iloc[:,7].map(occ_dictionary)#remap column 7\n",
    "mapped_col8 = idx_df.iloc[:,8].map(occ_dictionary)#remap column 8\n",
    "mapped_col9 = idx_df.iloc[:,9].map(occ_dictionary)#remap column 9\n",
    "\n",
    "#Zip all remapped columns and create a new dataframe which shows top 10 similar occupations for each occupation\n",
    "Final_occ_matches = pd.DataFrame(zip(mapped_col0, mapped_col1, mapped_col2, mapped_col3, mapped_col4, mapped_col5, mapped_col6, mapped_col7, mapped_col8, mapped_col9))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_occ_matches.loc[351]\n",
    "\n",
    "#Motor vehicles parts and accessories fitters index 351\n",
    "\n",
    "#occ_dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export 4 digit occupation file with cleaned associated words for use with Pyspark\n",
    "export_csv = occ4_base_data.to_csv (r'occ4_spark.csv', index = None, header=True, doublequote = True) #Don't forget to add '.csv' at the end of the path\n",
    "export_csv = occ4_base_data.to_csv (r'occ4_spark.txt', index = None, header=True ) #Don't forget to add '.csv' at the end of the path\n",
    "\n",
    "\n",
    "export_csv = occ4_spark.to_csv (r'occ4_spark_full.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf #actually use\n",
    "from operator import add #Actually use\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer# actualy use\n",
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud \n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start spark session\n",
    "sc.stop()\n",
    "conf = SparkConf()\\\n",
    "        .setAppName(\"Occupation_query\")\\\n",
    "        .setMaster(\"local[*]\")\\\n",
    "        .set(\"spark.driver.memory\", \"10g\")\\\n",
    "        .set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset into spark\n",
    "spark = SparkSession.builder.master('yarn-client').appName('Occupation_query_ds').getOrCreate()\n",
    "\n",
    "\n",
    "#Set schema for spark to attach to loaded data\n",
    "schema = StructType([\n",
    "    StructField(\"occupation code\", IntegerType(), True),\n",
    "    StructField(\"Associated words\", StringType(), True)])\n",
    "\n",
    "    \n",
    "#ArrayType(StructType([StructField(\"Associated words\", StringType()), True)]))\n",
    "\n",
    "#Read 4 digit occupation file with cleaned associated words into spark\n",
    "data_occ_spark = (spark.read.csv(\"/Users/preshantjoseph/Documents/University/Data Science Master Class 1/Assignment 4/occ4_base_data.csv\", \n",
    "                                 header = True, mode=\"DROPMALFORMED\", inferSchema = True\n",
    "                                 #schema = schema\n",
    "                                ))\n",
    "\n",
    "\n",
    "data_occ_spark.withColumn(\n",
    "    \"Associated words\",\n",
    "    split(col(\"Associated words\"), \",\\s*\").cast(\"array<string>\").alias(\"Associated words\")\n",
    ")\n",
    "\n",
    "data_occ_spark.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rawData = sc.textFile(r\"/Users/preshantjoseph/Documents/University/Data Science Master Class 1/Assignment 4/occ4_base_data.txt\")\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "documents = sc.textFile(\"/Users/preshantjoseph/Documents/University/Data Science Master Class 1/Assignment 4/occ4_base_data.txt\").map(lambda line: line.split(\"  \"))\n",
    "#titles = rawData.map(lambda line : line.split('\\t')[0])\n",
    "#titles.cache()\n",
    "\n",
    "#documents.take(1)\n",
    "hashingTF = HashingTF(numFeatures = 20000000)  #20 Million hash buckets just to make sure it fits in memory\n",
    "tf = hashingTF.transform(documents)\n",
    "#idf = IDF(minDocFreq=10).fit(tf)\n",
    "#tfidf = idf.transform(tf)\n",
    "#tfidf.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(conf=conf)\n",
    "rawData = data_occ_spark\n",
    "\n",
    "\n",
    "#documents = rawData.map(lambda line : line.split('\\t')[1].split())\n",
    "#docs = essays.map(lambda (filename, contents): contents)\n",
    "\n",
    "\n",
    "#documents = rawData.map(lambda line : line.split('\\t')[1].split())\n",
    "#titles = rawData.map(lambda line : line.split('\\t')[0])\n",
    "#titles.cache()\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Associated words\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(data_occ_spark)\n",
    "\n",
    "hashingTF = HashingTF(inputCol= \"words\", outputCol=\"rawFeatures\", numFeatures=20) #20 Million hash buckets just to make sure it fits in memory\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "featurizedData.cache()\n",
    "\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "#rescaledData = idfModel.transform(tf)\n",
    "\n",
    "#rescaledData.select(\"label\", \"features\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_occ_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QueryTF = hashingTF.transform([\"tabriz\"])\n",
    "QueryHashValue = QueryTF.indices[0]\n",
    "QueryRelevance = tfidf.map(lambda x: x[QueryHashValue])\n",
    "zippedResults = QueryRelevance.zip(titles)\n",
    "print \"Top 10 related documents:\"\n",
    "for (k, v) in zippedResults.sortByKey(ascending=False).take(10):\n",
    "    print v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
